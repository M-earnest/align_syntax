{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceffb7d9-320f-4957-ae4a-8dfcc982ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries needed for annotation\n",
    "import string as st\n",
    "import stanza\n",
    "import pandas as pd\n",
    "from nltk.tree import *\n",
    "from stanza.models.constituency.tree_reader import read_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f8e258-a719-4f06-8204-b660364aea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 16:16:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d353c08cde46c7b1837fbc41571807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 16:16:17 INFO: Downloaded file to /Users/test/stanza_resources/resources.json\n",
      "2024-12-06 16:16:17 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2024-12-06 16:16:17 INFO: Using device: cpu\n",
      "2024-12-06 16:16:17 INFO: Loading: tokenize\n",
      "2024-12-06 16:16:17 INFO: Loading: mwt\n",
      "2024-12-06 16:16:17 INFO: Loading: pos\n",
      "2024-12-06 16:16:17 INFO: Loading: constituency\n",
      "2024-12-06 16:16:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "nlp = stanza.Pipeline(lang= 'en', processors= 'tokenize, mwt, pos, constituency', \n",
    "                      use_gpu=(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "510bda59-edec-4fda-af93-8c7c713aba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_annotation(tree, words):\n",
    "    '''\n",
    "    tree is a constituency object coming from stanza. You can also import a string instead of the object.\n",
    "\n",
    "    words is a word object coming from stanza. Alternatively you can pass a list of words\n",
    "\n",
    "    Example\n",
    "    \n",
    "    import string as st\n",
    "    import stanza\n",
    "    import pandas as pd\n",
    "    from nltk.tree import *\n",
    "    *sentence is a string\n",
    "    doc =nlp(sentence)\n",
    "    \n",
    "    e.g., \n",
    "    tree = doc.sentences[j].constituency # j is the indext of the sentence to analyse\n",
    "    words = doc.sentences[j].words\n",
    "    '''\n",
    "    \n",
    "    #check if the input is a string or not\n",
    "    if isinstance(tree, str): \n",
    "\n",
    "        tree = read_trees(tree)[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    #import tree into nltk\n",
    "    tree_string = Tree.fromstring(str(tree))\n",
    "    \n",
    "    #get indexes for every terminal node\n",
    "    tpos = tree_string.treepositions('leaves')\n",
    "    \n",
    "    \n",
    "    #initialize list to store phrase extraction\n",
    "    #in this case, it will be a list of nexted lists\n",
    "    \n",
    "    all_labels = list()\n",
    "    \n",
    "    \n",
    "    #loop through the idx to get to every leaf\n",
    "    for leaf_idx in tpos: \n",
    "        \n",
    "        #initialize function to be used in Stanza\n",
    "        layer = tree.children\n",
    "        \n",
    "        #list of labels for this leaf, to nest into the other list\n",
    "        labels = []\n",
    "        \n",
    "\n",
    "\n",
    "        #loop through current leaf_idx\n",
    "        for n, i in enumerate(leaf_idx): \n",
    "            \n",
    "            #add current index\n",
    "            layer = layer[i]\n",
    "            \n",
    "            #extract labels\n",
    "            label = layer.label\n",
    "\n",
    "            #add current level of embedding\n",
    "            label_n = '/'\n",
    "            for idx in list(leaf_idx)[:n+1]: \n",
    "                label_n += str(idx) \n",
    "            \n",
    "            #add identifier\n",
    "            label += label_n\n",
    "            \n",
    "            #store label\n",
    "            labels.append(label)\n",
    "            \n",
    "            #prepare for next iteration\n",
    "            layer = layer.children\n",
    "\n",
    "            #function_string += '.children'\n",
    "        \n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    #store in a df and add words    \n",
    "    df = pd.DataFrame(all_labels)\n",
    "    \n",
    "    #chech if words is a list\n",
    "    if isinstance(words[0], str):\n",
    "\n",
    "        df.insert(loc=0, column='words', value=words)\n",
    "        \n",
    "        \n",
    "    else: \n",
    "        words = [w.text for w in words]\n",
    "        df.insert(loc=0, column='words', value=words) \n",
    "        \n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e262e188-edde-447b-95d3-d2eb9e54fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset from Brennan, 2023.\n",
    "# available here: https://deepblue.lib.umich.edu/data/concern/data_sets/bg257f92t\n",
    "\n",
    "path = 'Data/AliceChapterOne-EEG.csv'\n",
    "\n",
    "dfBrennan = pd.read_csv(path)\n",
    "\n",
    "#extract the n of sentences in the datagrame\n",
    "n_sents = set(dfBrennan.Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "091ecea2-b356-43f8-a36d-b5057b64a7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Segment</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>Order</th>\n",
       "      <th>LogFreq</th>\n",
       "      <th>LogFreq_Prev</th>\n",
       "      <th>LogFreq_Next</th>\n",
       "      <th>SndPower</th>\n",
       "      <th>Length</th>\n",
       "      <th>Position</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>IsLexical</th>\n",
       "      <th>NGRAM</th>\n",
       "      <th>RNN</th>\n",
       "      <th>CFG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.608721</td>\n",
       "      <td>1</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.56</td>\n",
       "      <td>3.621500e-07</td>\n",
       "      <td>0.562721</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.226499</td>\n",
       "      <td>3.126175</td>\n",
       "      <td>2.312348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>1</td>\n",
       "      <td>0.562721</td>\n",
       "      <td>0.830543</td>\n",
       "      <td>2</td>\n",
       "      <td>14.56</td>\n",
       "      <td>8.65</td>\n",
       "      <td>10.69</td>\n",
       "      <td>3.843500e-09</td>\n",
       "      <td>0.267822</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.905229</td>\n",
       "      <td>1.691128</td>\n",
       "      <td>1.357460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beginning</td>\n",
       "      <td>1</td>\n",
       "      <td>0.784543</td>\n",
       "      <td>1.302929</td>\n",
       "      <td>3</td>\n",
       "      <td>10.69</td>\n",
       "      <td>14.56</td>\n",
       "      <td>16.35</td>\n",
       "      <td>3.686500e-09</td>\n",
       "      <td>0.518386</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.446766</td>\n",
       "      <td>4.100771</td>\n",
       "      <td>5.626722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>1</td>\n",
       "      <td>1.256929</td>\n",
       "      <td>1.398925</td>\n",
       "      <td>4</td>\n",
       "      <td>16.35</td>\n",
       "      <td>10.69</td>\n",
       "      <td>13.79</td>\n",
       "      <td>3.969700e-09</td>\n",
       "      <td>0.141996</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.537495</td>\n",
       "      <td>3.833313</td>\n",
       "      <td>5.939201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get</td>\n",
       "      <td>1</td>\n",
       "      <td>1.352925</td>\n",
       "      <td>1.662327</td>\n",
       "      <td>5</td>\n",
       "      <td>13.79</td>\n",
       "      <td>16.35</td>\n",
       "      <td>13.28</td>\n",
       "      <td>3.774700e-09</td>\n",
       "      <td>0.309402</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.023137</td>\n",
       "      <td>1.013076</td>\n",
       "      <td>2.697304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>happens</td>\n",
       "      <td>12</td>\n",
       "      <td>45.226353</td>\n",
       "      <td>45.672448</td>\n",
       "      <td>2146</td>\n",
       "      <td>10.77</td>\n",
       "      <td>10.82</td>\n",
       "      <td>13.76</td>\n",
       "      <td>7.081200e-04</td>\n",
       "      <td>0.446095</td>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.574428</td>\n",
       "      <td>6.356812</td>\n",
       "      <td>2.969568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>when</td>\n",
       "      <td>12</td>\n",
       "      <td>45.677924</td>\n",
       "      <td>45.891353</td>\n",
       "      <td>2147</td>\n",
       "      <td>13.76</td>\n",
       "      <td>10.77</td>\n",
       "      <td>14.17</td>\n",
       "      <td>3.221100e-03</td>\n",
       "      <td>0.213429</td>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.059164</td>\n",
       "      <td>6.720639</td>\n",
       "      <td>4.930669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>one</td>\n",
       "      <td>12</td>\n",
       "      <td>45.896829</td>\n",
       "      <td>46.058972</td>\n",
       "      <td>2148</td>\n",
       "      <td>14.17</td>\n",
       "      <td>13.76</td>\n",
       "      <td>8.15</td>\n",
       "      <td>1.984200e-03</td>\n",
       "      <td>0.162143</td>\n",
       "      <td>8</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.380381</td>\n",
       "      <td>2.187682</td>\n",
       "      <td>0.725398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>eats</td>\n",
       "      <td>12</td>\n",
       "      <td>46.064448</td>\n",
       "      <td>46.322373</td>\n",
       "      <td>2149</td>\n",
       "      <td>8.15</td>\n",
       "      <td>14.17</td>\n",
       "      <td>8.74</td>\n",
       "      <td>2.473000e-05</td>\n",
       "      <td>0.257925</td>\n",
       "      <td>9</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.171368</td>\n",
       "      <td>3.941021</td>\n",
       "      <td>2.767965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>cake</td>\n",
       "      <td>12</td>\n",
       "      <td>46.327849</td>\n",
       "      <td>46.681557</td>\n",
       "      <td>2150</td>\n",
       "      <td>8.74</td>\n",
       "      <td>8.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.423800e-06</td>\n",
       "      <td>0.353708</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.386008</td>\n",
       "      <td>5.832085</td>\n",
       "      <td>3.692958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2129 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Segment      onset     offset  Order  LogFreq  LogFreq_Prev  \\\n",
       "0         Alice        1   0.046000   0.608721      1     8.65          0.00   \n",
       "1           was        1   0.562721   0.830543      2    14.56          8.65   \n",
       "2     beginning        1   0.784543   1.302929      3    10.69         14.56   \n",
       "3            to        1   1.256929   1.398925      4    16.35         10.69   \n",
       "4           get        1   1.352925   1.662327      5    13.79         16.35   \n",
       "...         ...      ...        ...        ...    ...      ...           ...   \n",
       "2124    happens       12  45.226353  45.672448   2146    10.77         10.82   \n",
       "2125       when       12  45.677924  45.891353   2147    13.76         10.77   \n",
       "2126        one       12  45.896829  46.058972   2148    14.17         13.76   \n",
       "2127       eats       12  46.064448  46.322373   2149     8.15         14.17   \n",
       "2128       cake       12  46.327849  46.681557   2150     8.74          8.15   \n",
       "\n",
       "      LogFreq_Next      SndPower    Length  Position  Sentence  IsLexical  \\\n",
       "0            14.56  3.621500e-07  0.562721         1         1        1.0   \n",
       "1            10.69  3.843500e-09  0.267822         2         1        0.0   \n",
       "2            16.35  3.686500e-09  0.518386         3         1        1.0   \n",
       "3            13.79  3.969700e-09  0.141996         4         1        0.0   \n",
       "4            13.28  3.774700e-09  0.309402         5         1        0.0   \n",
       "...            ...           ...       ...       ...       ...        ...   \n",
       "2124         13.76  7.081200e-04  0.446095         6        84        1.0   \n",
       "2125         14.17  3.221100e-03  0.213429         7        84        0.0   \n",
       "2126          8.15  1.984200e-03  0.162143         8        84        1.0   \n",
       "2127          8.74  2.473000e-05  0.257925         9        84        1.0   \n",
       "2128          0.00  5.423800e-06  0.353708        10        84        1.0   \n",
       "\n",
       "         NGRAM       RNN       CFG  \n",
       "0     3.226499  3.126175  2.312348  \n",
       "1     0.905229  1.691128  1.357460  \n",
       "2     4.446766  4.100771  5.626722  \n",
       "3     2.537495  3.833313  5.939201  \n",
       "4     1.023137  1.013076  2.697304  \n",
       "...        ...       ...       ...  \n",
       "2124  5.574428  6.356812  2.969568  \n",
       "2125  4.059164  6.720639  4.930669  \n",
       "2126  1.380381  2.187682  0.725398  \n",
       "2127  3.171368  3.941021  2.767965  \n",
       "2128  5.386008  5.832085  3.692958  \n",
       "\n",
       "[2129 rows x 16 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBrennan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24b87fd5-bedc-4d5b-bea5-cba16fa5b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize empty lists where to store the annotations\n",
    "f_dfs = []\n",
    "dfSents = []\n",
    "\n",
    "#loop for every sentence, instead of doing 1 analysis for all the dataframe\n",
    "for idx_sent in list(n_sents): \n",
    "\n",
    "    #slice the for every sentece\n",
    "    dfSent = dfBrennan[dfBrennan.Sentence == idx_sent]\n",
    "    dfSent = dfSent.reset_index(drop = True)\n",
    "\n",
    "    #extract the words for the current sentence\n",
    "    current_words = dfSent.Word\n",
    "    \n",
    "    sent_to_analyse = [' '.join(w for w in current_words)]\n",
    "\n",
    "    #store dfSent for comparison with the annotation\n",
    "    dfSents.append(dfSent)\n",
    "    \n",
    "    #analyse sent into a tree \n",
    "    \n",
    "    doc =nlp(sent_to_analyse[0])\n",
    "\n",
    "    #check for the length of the parsed document. This ensures that \n",
    "    #the parsed sentences will later align with the sentences as in Brennan et al's annotations\n",
    "    if len(doc.sentences) == 1: \n",
    "\n",
    "        #parse into a stanza tree and extract current words\n",
    "        \n",
    "        tree = doc.sentences[0].constituency # this could be also a string coming from another model\n",
    "        words = doc.sentences[0].words # this could be a list of words as strings\n",
    "        \n",
    "        # run syntactic analysis for the current sentence\n",
    "        df_current_sent = syntactic_annotation(tree, words)\n",
    "        \n",
    "        #insert words for comparison. Note that number of words could be \n",
    "        #different in the case of multi-word tokenization\n",
    "        df_current_sent.insert(loc=1, column='idx_sent', value=[\n",
    "            'correct' for i in range(len(df_current_sent))]) # correct is to make sure we match original sent index\n",
    "        df_current_sent = df_current_sent.reset_index()\n",
    "       \n",
    "\n",
    "    # in case the parsed doc contains more the one sentence\n",
    "    if len(doc.sentences) > 1: \n",
    "\n",
    "        #store all sentences in the current doc\n",
    "        storedfs = []\n",
    "\n",
    "        #loop through the current sentences\n",
    "        for j, sent in enumerate(doc.sentences):\n",
    "\n",
    "            #parse into a tree for every sentence\n",
    "            tree = sent.constituency\n",
    "            words = sent.words\n",
    "\n",
    "            #run syntactic analysis\n",
    "            df_current_sent = syntactic_annotation(tree, words)\n",
    "\n",
    "            \n",
    "            df_current_sent.insert(loc=1, column='idx_sent', value=[\n",
    "                str(idx_sent) + f'_{j+1}' \n",
    "                for i in range(len(df_current_sent))]) # append a subindex for the current sentence\n",
    "            storedfs.append(df_current_sent)\n",
    "        \n",
    "        df_current_sent = pd.concat(storedfs)\n",
    "        df_current_sent = df_current_sent.reset_index(drop = True)\n",
    "        #df_current_sent = pd.concat([ df_current_sent, dfSent,], axis = 1, ignore_index=True)\n",
    "        \n",
    "    f_dfs.append(df_current_sent)\n",
    "    \n",
    "    \n",
    "alldf = pd.concat(f_dfs, ignore_index= True)\n",
    "\n",
    "# for the purpose of this, it is not need to write any more complex code\n",
    "alldf = alldf[alldf.words != \"'s\"].reset_index(drop = True)\n",
    "allsents = pd.concat(dfSents, ignore_index= True)\n",
    "\n",
    "#final df\n",
    "df_aligned = pd.concat([alldf, allsents], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26ed70cb-a0ec-4fcb-9253-0f58f059b7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>words</th>\n",
       "      <th>idx_sent</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>LogFreq_Prev</th>\n",
       "      <th>LogFreq_Next</th>\n",
       "      <th>SndPower</th>\n",
       "      <th>Length</th>\n",
       "      <th>Position</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>IsLexical</th>\n",
       "      <th>NGRAM</th>\n",
       "      <th>RNN</th>\n",
       "      <th>CFG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Alice</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>NP/00</td>\n",
       "      <td>NNP/000</td>\n",
       "      <td>Alice/0000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.56</td>\n",
       "      <td>3.621500e-07</td>\n",
       "      <td>0.562721</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.226499</td>\n",
       "      <td>3.126175</td>\n",
       "      <td>2.312348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>was</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/01</td>\n",
       "      <td>VBD/010</td>\n",
       "      <td>was/0100</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>8.65</td>\n",
       "      <td>10.69</td>\n",
       "      <td>3.843500e-09</td>\n",
       "      <td>0.267822</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.905229</td>\n",
       "      <td>1.691128</td>\n",
       "      <td>1.357460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>beginning</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/01</td>\n",
       "      <td>VP/011</td>\n",
       "      <td>VP/0110</td>\n",
       "      <td>VBG/01100</td>\n",
       "      <td>beginning/011000</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>14.56</td>\n",
       "      <td>16.35</td>\n",
       "      <td>3.686500e-09</td>\n",
       "      <td>0.518386</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.446766</td>\n",
       "      <td>4.100771</td>\n",
       "      <td>5.626722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>to</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/01</td>\n",
       "      <td>VP/011</td>\n",
       "      <td>VP/0110</td>\n",
       "      <td>S/01101</td>\n",
       "      <td>VP/011010</td>\n",
       "      <td>TO/0110100</td>\n",
       "      <td>...</td>\n",
       "      <td>10.69</td>\n",
       "      <td>13.79</td>\n",
       "      <td>3.969700e-09</td>\n",
       "      <td>0.141996</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.537495</td>\n",
       "      <td>3.833313</td>\n",
       "      <td>5.939201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>get</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/01</td>\n",
       "      <td>VP/011</td>\n",
       "      <td>VP/0110</td>\n",
       "      <td>S/01101</td>\n",
       "      <td>VP/011010</td>\n",
       "      <td>VP/0110101</td>\n",
       "      <td>...</td>\n",
       "      <td>16.35</td>\n",
       "      <td>13.28</td>\n",
       "      <td>3.774700e-09</td>\n",
       "      <td>0.309402</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.023137</td>\n",
       "      <td>1.013076</td>\n",
       "      <td>2.697304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>5.0</td>\n",
       "      <td>happens</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/00</td>\n",
       "      <td>VP/001</td>\n",
       "      <td>ADJP/0011</td>\n",
       "      <td>SBAR/00111</td>\n",
       "      <td>S/001110</td>\n",
       "      <td>VP/0011102</td>\n",
       "      <td>...</td>\n",
       "      <td>10.82</td>\n",
       "      <td>13.76</td>\n",
       "      <td>7.081200e-04</td>\n",
       "      <td>0.446095</td>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.574428</td>\n",
       "      <td>6.356812</td>\n",
       "      <td>2.969568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>6.0</td>\n",
       "      <td>when</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/00</td>\n",
       "      <td>VP/001</td>\n",
       "      <td>ADJP/0011</td>\n",
       "      <td>SBAR/00111</td>\n",
       "      <td>S/001110</td>\n",
       "      <td>VP/0011102</td>\n",
       "      <td>...</td>\n",
       "      <td>10.77</td>\n",
       "      <td>14.17</td>\n",
       "      <td>3.221100e-03</td>\n",
       "      <td>0.213429</td>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.059164</td>\n",
       "      <td>6.720639</td>\n",
       "      <td>4.930669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>7.0</td>\n",
       "      <td>one</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/00</td>\n",
       "      <td>VP/001</td>\n",
       "      <td>ADJP/0011</td>\n",
       "      <td>SBAR/00111</td>\n",
       "      <td>S/001110</td>\n",
       "      <td>VP/0011102</td>\n",
       "      <td>...</td>\n",
       "      <td>13.76</td>\n",
       "      <td>8.15</td>\n",
       "      <td>1.984200e-03</td>\n",
       "      <td>0.162143</td>\n",
       "      <td>8</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.380381</td>\n",
       "      <td>2.187682</td>\n",
       "      <td>0.725398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>8.0</td>\n",
       "      <td>eats</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/00</td>\n",
       "      <td>VP/001</td>\n",
       "      <td>ADJP/0011</td>\n",
       "      <td>SBAR/00111</td>\n",
       "      <td>S/001110</td>\n",
       "      <td>VP/0011102</td>\n",
       "      <td>...</td>\n",
       "      <td>14.17</td>\n",
       "      <td>8.74</td>\n",
       "      <td>2.473000e-05</td>\n",
       "      <td>0.257925</td>\n",
       "      <td>9</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.171368</td>\n",
       "      <td>3.941021</td>\n",
       "      <td>2.767965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>9.0</td>\n",
       "      <td>cake</td>\n",
       "      <td>correct</td>\n",
       "      <td>S/0</td>\n",
       "      <td>VP/00</td>\n",
       "      <td>VP/001</td>\n",
       "      <td>ADJP/0011</td>\n",
       "      <td>SBAR/00111</td>\n",
       "      <td>S/001110</td>\n",
       "      <td>VP/0011102</td>\n",
       "      <td>...</td>\n",
       "      <td>8.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.423800e-06</td>\n",
       "      <td>0.353708</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.386008</td>\n",
       "      <td>5.832085</td>\n",
       "      <td>3.692958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2129 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      words idx_sent    0      1        2           3           4  \\\n",
       "0       0.0      Alice  correct  S/0  NP/00  NNP/000  Alice/0000        None   \n",
       "1       1.0        was  correct  S/0  VP/01  VBD/010    was/0100        None   \n",
       "2       2.0  beginning  correct  S/0  VP/01   VP/011     VP/0110   VBG/01100   \n",
       "3       3.0         to  correct  S/0  VP/01   VP/011     VP/0110     S/01101   \n",
       "4       4.0        get  correct  S/0  VP/01   VP/011     VP/0110     S/01101   \n",
       "...     ...        ...      ...  ...    ...      ...         ...         ...   \n",
       "2124    5.0    happens  correct  S/0  VP/00   VP/001   ADJP/0011  SBAR/00111   \n",
       "2125    6.0       when  correct  S/0  VP/00   VP/001   ADJP/0011  SBAR/00111   \n",
       "2126    7.0        one  correct  S/0  VP/00   VP/001   ADJP/0011  SBAR/00111   \n",
       "2127    8.0       eats  correct  S/0  VP/00   VP/001   ADJP/0011  SBAR/00111   \n",
       "2128    9.0       cake  correct  S/0  VP/00   VP/001   ADJP/0011  SBAR/00111   \n",
       "\n",
       "                     5           6  ... LogFreq_Prev LogFreq_Next  \\\n",
       "0                 None        None  ...         0.00        14.56   \n",
       "1                 None        None  ...         8.65        10.69   \n",
       "2     beginning/011000        None  ...        14.56        16.35   \n",
       "3            VP/011010  TO/0110100  ...        10.69        13.79   \n",
       "4            VP/011010  VP/0110101  ...        16.35        13.28   \n",
       "...                ...         ...  ...          ...          ...   \n",
       "2124          S/001110  VP/0011102  ...        10.82        13.76   \n",
       "2125          S/001110  VP/0011102  ...        10.77        14.17   \n",
       "2126          S/001110  VP/0011102  ...        13.76         8.15   \n",
       "2127          S/001110  VP/0011102  ...        14.17         8.74   \n",
       "2128          S/001110  VP/0011102  ...         8.15         0.00   \n",
       "\n",
       "          SndPower    Length Position Sentence IsLexical     NGRAM       RNN  \\\n",
       "0     3.621500e-07  0.562721        1        1       1.0  3.226499  3.126175   \n",
       "1     3.843500e-09  0.267822        2        1       0.0  0.905229  1.691128   \n",
       "2     3.686500e-09  0.518386        3        1       1.0  4.446766  4.100771   \n",
       "3     3.969700e-09  0.141996        4        1       0.0  2.537495  3.833313   \n",
       "4     3.774700e-09  0.309402        5        1       0.0  1.023137  1.013076   \n",
       "...            ...       ...      ...      ...       ...       ...       ...   \n",
       "2124  7.081200e-04  0.446095        6       84       1.0  5.574428  6.356812   \n",
       "2125  3.221100e-03  0.213429        7       84       0.0  4.059164  6.720639   \n",
       "2126  1.984200e-03  0.162143        8       84       1.0  1.380381  2.187682   \n",
       "2127  2.473000e-05  0.257925        9       84       1.0  3.171368  3.941021   \n",
       "2128  5.423800e-06  0.353708       10       84       1.0  5.386008  5.832085   \n",
       "\n",
       "           CFG  \n",
       "0     2.312348  \n",
       "1     1.357460  \n",
       "2     5.626722  \n",
       "3     5.939201  \n",
       "4     2.697304  \n",
       "...        ...  \n",
       "2124  2.969568  \n",
       "2125  4.930669  \n",
       "2126  0.725398  \n",
       "2127  2.767965  \n",
       "2128  3.692958  \n",
       "\n",
       "[2129 rows x 52 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01149b84-fc52-4627-9981-7f76b8ab45f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415b2fd-e748-49a9-833c-35539749f4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb51c9-64e2-4aef-bab0-b5780e25c19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
